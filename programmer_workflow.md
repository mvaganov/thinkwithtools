## The unavoidable role of testing in software development
Understanding in software development comes from evidence, not intention. Code is understood when you have an expectation, and then observe how it actually behaves. This process, testing a hypothesis against real data, is foundational to programming.

Testing is best understood as hypothesis validation. Make a guess about how a system works, introduce inputs or stress, observe the outputs, and compare the results to expectations. Over time, this cycle builds the practical judgment and technical intuition that distinguishes productive programmers.

LLMs can unintentionally create the impression that testing is optional. This is a mental hazard. Testing and debugging are highly situational, often tedious, and inconsistently documented. Under-representation in written material leads LLMs to under-represent the importance of testing. This is not deception or intent; it is a limitation of the medium.

Testing remains non-optional even when LLMs are used. Writing, compiling, running, observing, and validating code behavior is how understanding is earned. Testing skills are not automatically acquired, and they are not transferred by tools. They are developed through repeated practice and deliberate skepticism of assumptions.

The ability to design tests is practiced by beginners (printing variables), and is an implicit professional expectation of programmers (also called telemetry). Without this discipline, progress can be misleading. Much of the guidance in the rest of this document assumes that testing is treated as a core part of thinking, not as a secondary or deferrable step.

# LLM Workflow for Software Projects

## Use it early
* Ask LLMs questions about concepts you don't understand, and do it soon. The value of understanding compounds over time, and confusion is a "Technical Debt" that also compounds over time. An LLM can usually give insight that is missing from code or documentation (even this document).
* Use your new knowledge early. After you learn something new, validate that your understanding makes sense with a test.
* Because of the nature of software development, the highest-leverage point of intelligence is earlier in the process (eg: a bug is always cheaper to fix before the product is released; and it's cheaper to design a bug out in the design phase; and it's cheaper to eliminate the problem/solution spaces in the conceptual phase). Because of this, the LLM should absolutely be used early in the conceptual phase of software development.
* Do your own research and write your own first draft before asking the LLM to critique it. If you ask the LLM to generate the first draft, it will never feel like your work, and that lack of ownership will corrupt the your workflow from the beginning.
* Provide a lot of context on the goals of the project early in your prompts. if you have a large amount of context, ask the LLM to summarize it for you, edit that summary as needed, and provide those concise goals to the LLM during each new interaction.
* Having an LLM critique your thoughts is an extremely high leverage use of artificial intelligence, and can lead to significant personal growth. When asking the LLM for critique, explicitly ask "give no encouraging feedback *except for* feedback that will encourage [the project's simple ultimate goal]". This is a guard to prevent the LLM from distracting you. Make sure you have a clear understanding of the project's simple and ultimate goal(s) before you ask the LLM for feedback.
* Be ready to start over if the LLM helps you learn some important points that you didn't consider in your initial design. Before you start over, ask the LLM to summarize your best points from your existing work, and then start over. This process is so much faster with machine tooling than without it, and the step-change in quality improvement is probably significant enough to justify a setback. Try it. But, be mindful about integration (when software starts interacting with systems beyond itself). That part of software development is always painful, always slow (because of testing), and difficult to truly understand until you are finished.

## Use it to launch you into productivity
Once you feel good about the purpose and general design of your project, use the LLM to help start your project:
* With the project's goals and the system's responsibilities in mind, ask the LLM for a minimal boiler-plate starting point for your project. Before you ask, you should know how to start a project yourself, and then take advantage of automation that will go through the setup/configuration for you. Get that boiler-plate compiling and running ASAP, and then read through it to make sure it is what you expect. Your power as a developer is expressed in the implementation/testing iterative loop, and LLM automation can help you get into that loop sooner.
* Using an LLM to explain something you don't understand is an extremely high leverage use of artificial intelligence. Ask the LLM to explain things in the boilerplate that you don't understand, and find tutorials/documentation online that give you additional context. There are many good reasons that experts stress "fundamentals", so don't be afraid to use LLMs to polish these skills. "explain [this_code_or_concept] like I'm a junior developer". Remember to test your new understanding: explanations feel like understanding, and need to be confirmed by observation.
* If you have trouble getting the boilerplate code working, ask the LLM to help you debug it with you. Describe your problem in detail, and provide exact error messages wherever possible.
* If you don't have a clear path forward, Test Driven Development is a solid approach (write Unit Tests before you write the functionality, ask an LLM to explain Test Driven Development if you need). LLMs are able to write Unit Tests quickly, which make project tasks very visible and even motivating. Give the LLM your context, and ask it to write unit tests in a Test Driven Development style. Make sure you provide meaning/intention for the unit tests when they are generated.

## Build your own mental muscle
* Over-reliance on LLMs will reduce the strength of your own mind. Try to do the mental "lift" on your own first. When you feel stuck because you have tried your own solutions and there seems to be a magical piece of missing knowledge, that is a great time to prompt an LLM for help. Try to understand why your current attempt fails, and remember to create telemetry (debug statements) to test and confirm your assumptions. That information will be excellent additional context to help the LLM work through a solution with you. Also, with the mental scaffolding you build in that time, you will be more ready to understand the solution that the LLM gives you.
* If you are a hobbyist without a paid LLM usage plan, you will use up your free LLM compute tokens very quickly by asking lots of small questions. You will improve your own understanding and get better use from compute tokens by asking high-leverage questions with lots of context.
* If you don't want the LLM to use compute tokens answering a specific question for you, answer it first as part of the context for your prompt. This exercise of articulating your situation very clearly is an effective debugging technique called "Rubber Ducky Debugging", and can help you answer your own questions without the aid of the LLM. Spend tokens to improve understanding instead of avoiding consideration.
* As you become a more skilled developer, understanding becomes less of a bottleneck, and that understanding mitigates risks from LLM use. Senior developers will use LLMs differently because their bottleneck shifts: typing and testing speed becomes the bottleneck instead of understanding and compute tokens. Senior devs wish they could type faster (or had more time), they are instead forced to use LLMs because of an oversupply of understanding. Importantly, Judgement remains as a bottleneck, and senior developers focus on making judgement more efficient to dispense.
* Don't assume LLM generated code is production ready, certainly not in the first pass. LLMs often prioritize functionality over security and maintainability. Review it like any other code.
* Testing does not mean "it ran once without crashing", It means deliberately trying to falsify your expectations, and the expectations of the LLM. 

## NEVER use LLMs for certain things
* Pasting API keys, database credentials, or proprietary PII (Personally Identifiable Information) into an LLM on the public internet can lead to termination at many companies. Be responsible, sanitize your context.
* Notice: I am not a lawyer, this is not legal advice.
* Your chats with LLMs are searchable by the companies providing the LLM service. Those companies can theoretically read and sell your data. Those companies can also be compelled by law enforcement or state and local governments to give up your de-anonymized data. It's best to keep that in mind. Don't share company secrets with the LLM, or create the illusion of unethical behavior.
* Don't use complete solutions provided by an LLM without any modification. Intellectual Property law complexities make using exact copies risky. Create "transformative" work by making your own modifications, to mitigate IP concerns.

## Use it to illuminate confusion
While you are deep in the middle of writing code and stuck on an algorithm:
* Describe the algorithm that you need to the LLM. Provide context specific to your project, including your initial implementation (code).
* Always try to refactor example code you have a question about to use as few dependencies as possible. This is also a debugging technique called "Minimal Reproducible Example". Extra dependencies/files will distract the model, essentially hurting the signal-to-noise ratio of your prompt. More noise reduces the amount of intelligence the LLM can apply to actual problem solving.
* LLMs are can help you write test code, or at the very least, help you brainstorm what kind of tests you need to write. Writing Unit Tests in particular is a high-leverage use of LLMs.
* Ask the LLM for a code review, this is an extremely high leverage use of artificial intelligence: "Review this code as if you were a Senior Developer looking for style inconsistencies, potential bugs and edge cases, readability improvements, and performance bottlenecks." This can help you catch embarrassing mistakes, and offer insight into how your programming can improve.
* When an LLM gives what feels like too much feedback, remember that you can read the feedback and apply it at your own pace. Instead of feeling overwhelmed, recognize the value of having clear goals, and context-aware suggestions that you are free to disagree with. Remember that an LLM is not an adversary or boss that is controlling you, it is a tool to help you think.

## Common LLM Anti-Patterns
* Prompting for full solutions before understanding the problem --> outsources your intelligence, prevents you from forming transferable abstractions. Instead, keep your mind engaged by decomposing problems and asking the LLM to consider specifics with you.
* More text improves an answer --> you can easily distracting LLMs from problem solving with noise. Context should improve signal, not noise.
* Iterating prompts instead of code --> LLMs are not deterministic compilers, using them that way is error-prone. Instead, use your programming skill, your critical literacy, and control the machine directly.
* Trusting green tests without reading code --> computer software, even LLMs, won't always do what you want. Assume ownership of code you commit, make sure it is what you want; let your wisdom be the final test.
* Using LLMs to avoid documentation entirely --> poorly considered logic and poor naming is technical debt, which will probably harm the project later. Write syntax to document itself, annotate with comments describing purpose and indented use.
* Assuming tests written by LLMs are meaningful --> automatically written tests do not always test actual intent, some tests can encode & perpetuate bugs. Audit tests to make ensure they test intent, not syntax, not a mirror of the implementation, and not enforcement of circumstances from a prototyping environment.
* Assuming code will work without testing on the target hardware --> LLMs are risky to use for writing software on poorly documented hardware. Mitigate that risk with additional testing.
* Accepting explanations without verifying behavior --> LLMs can and do hallucinate, seeming very confident as they do it. Be skeptical, and test the output, especially for something you didn't know before the LLM taught you. Replace "that sounds right" with "I observed it". Also, documentation could be incorrect, and an LLM could have no way of knowing.
* Accepting LLM abstractions --> LLMs write class hierarchies and patterns without understanding why they exist. Use your human experience to question LLM choices, and discuss those choices.
* Debugging with insufficient description --> LLMs will spend lots of energy trying to generate missing context, and may solve the wrong problem, or hallucinate a solution to a problem that never existed to begin with. Provide as much context for technical problems as possible: error messages, stack traces, function intent, software goals, and concrete data.
* Using LLMs to avoid reading primary sources --> LLMs are not perfect, are not always fully aligned with your goals, and can be confused. If you have access to primary source material, read it. It could be written by humans for humans, and might teach you better/faster/more-cheaply than an LLM.
* Repeated micro questions --> careless use of compute tokens has a negative impact on the LLM system. An LLM query on a premier LLM has real economic and environmental costs. Build a habit of developing questions that are worth asking.
* Syntax trivia --> LLMs are not as good (fast/efficient) at verifying code as a compiler running on your own computer. Use your own computer to answer questions wherever you can.
* Asking the same question with micro-edits --> like micro questions point, this squanders LLM resources. Be thoughtful about your question before you ask it. If you don't know what exact question you want to ask, explain your goal to the LLM and ask it to help you write the prompt.

## Strengths of different LLMs (Jan 2026)
These oversimplified descriptions will quickly become obsolete. You should try different models yourself, whatever the most popular models of the day are, to develop your own understanding. Also, if one model gets stuck on your project, it's likely another model will not get stuck in the same way, because of differences in model weights. Assume all models have been trained on computer programming, since it's known that training a model on programming improves their reasoning abilities.
### ChatGPT (OpenAI)
*The Generalist*, excellent at conversational reasoning and broad knowledge. It is capable at "logic puzzles" and breaking down abstract concepts. Best for brainstorming architecture, explaining difficult concepts ("Explain this like I'm 5"), and general debugging. If you need to have a back-and-forth conversation to understand why something works, start here.
### Grok
*The Skeptic*, has a less filtered personality. Best for adversarial testing. If you want a "second opinion" that might not align with the standard corporate-safe consensus, or you want to test how robust your logic is against a someone more critical.
### Claude (Anthropic)
*The Coder*, considered the gold standard for software development use-cases. Adheres to instructions and provides high-quality code generation. Best for complex refactoring, writing unit tests, and following very specific formatting rules. Tends to be less "chatty" and more precise with syntax than others. Also offers more Agentic features for programming (as of Jan 2026).
### Gemini (Google)
*The Researcher*, distinguishes itself with a massive context window and live internet integration. Best for "Read the Manual" tasks. Paste entire documentation libraries, massive files, or complete codebases/repositories, and ask questions. Also excellent for checking if code uses up-to-date libraries versus deprecated ones.

### Additional thoughts on LLM options
I have not used other LLMs enough to have a strong opinion. There will be disruption in this space soon, there always is.

## General tips for prompting
* Give a lot of context.
* If you don't know what you want, give the LLM your goals and ask the LLM to interview you to discover what you want.
* LLMs can sometimes be very wrong. For example, LLM Hallucination can cause it to believe in API calls that don't exist. Also, LLMs only understand APIs that were included in their training data set, and can suggest using deprecated API calls.
* LLMs don't have a context for the real world, and how software can have real impacts on physical reality (eg robotics, hardware prototyping). Poorly documented circumstances are blind-spots for LLMs. Implement and test complex code incrementally when physical assets could be harmed by malfunction.
* LLMs are not real intelligence, and don't have feelings. You don't need to feel justified to answer its questions or defend yourself against its potentially incorrect assertions.
* If you keep your mind engaged, your mind will grow stronger and you will be smarter. Skills you practice compound. Skills you don't practice will decay.
* Senior devs care most about "ownership". Once you hit 'Commit', you are the owner of that code. If it breaks in production, 'the AI wrote it' is not an acceptable excuse. If you can't explain it, don't commit it. Work with the LLM until you understand.  Ownership means you can explain, modify, and debug code without the LLM.
